## Predictive Modeling {.page_break_before}
   
__Machine learning methods__ 
<p style="text-align:justify;">
   Predictive modeling using machine learning was performed using four methods: linear regression, decision tree, random forest regression, and neural network. The dataset, which has 369 data in eight variables, was used for training. The validity of the model was confirmed visually by plotting it linearly and checking the RMSE and R2 values.
   RMSE is a root mean square error, in which the best value is 0 and the worst value is near infinite [7-8]. As the range of the criteria is infinite, it is difficult to say at which point the data has a good prediction. However, the data is well-trained when the value is near 0. R2 is the proportion of the variance in the dependent variable that is predictable from the independent variables [7-8]. R2 is best when close to 1 and worst in - infinite. Each machine learning method was compared with these criteria to show their effectiveness.


__1. Linear regression__  
<p style="text-align:justify;">
The first method used for machine learning was the linear regression model. A simple linear relationship was used for training, which gave a sense of data analysis and assisted in planning future machine learning methods. The mean squared error was used to minimize the error, and with seven variables used, each variable was multiplied by a random seed and added with random bias initially. The training was performed and plotted using the normalized data, as shown in Figure XX. The procedure for coding is shown in the appendix. The accuracy of the machine learning was validated by calculating R2 and RMSE values. R2 value was -3.4 and RMSE value was 4.3.

</p>
<figure style="text-align: center;">
    <img src="https://github.com/uiceds/project-team492/blob/main/content/images/Pic_1.png?raw=true" alt="Sample Image">
    <figcaption><strong>Figure XX1:</strong> Comparison between the NO2 values predicted from Linear Regression model and the observed NO2 values </figcaption>
</figure>


</p>
__2. Decision tree__
<p style="text-align:justify;">


<figure style="text-align: center;">
    <img src="https://github.com/uiceds/project-team492/blob/main/content/images/Pro3_6.jpg?raw=true" alt="Sample Image">
    <figcaption><strong>Figure XX2:</strong> Gini plot shown for selected six variables </figcaption>
</figure>


__3. Random forest Regressor__
<p style="text-align:justify;">   
Random forest is an ensemble learning method that aggregates predictions from multiple decision trees to reduce overfitting and improve generalization. Some key parameters of this algorithm are:
</p>
<p style="text-align:justify;">    
a) n_trees: It specifies the number of trees in the forest. A large number improves the stability and reduces variance but increases computational cost. Considering all these factors, the value of n_trees were considered 200 in this project.
</p>
<p style="text-align:justify;">  
b) max_depth: It limits the minimum number of samples required to split and internal node. A depth of 20 is considered in this study which is suitable for capturing complex pattern of land use pattern and correlation with NO2 data without overfitting.
</p>
<p style="text-align:justify;">  
c) min_samples_split: It is the value of minimum number of samples that are required to split an internal node. The large values prevent overfitting by avoiding overly specific splits. A value of 15 was considered in the model.
</p>
<p style="text-align:justify;">  
d) min_samples_leaf: The minimum number of samples required to be at a leaf node which ensures that leaf nodes represent significant data, improving model generalization. 

<p style="text-align:justify;">  
The features were standardized by subtracting the mean and dividing by the standard deviation for each column. This ensures that all features are on the same scale, preventing the dominance of features with large numerical ranges during model training. Julia's DecisionTree package was used for the analysis. A typical 80/20 split for training and testing is used to evaluate the model's generalization on unseen data. As an evaluation metric, means squared error (MSE), root mean squared error (RMSE), and R2 score were used to assess the accuracy of the predictive modeling. For the random forest regression model, the RMSE value was 3.58, and the R2 value was 0.54.
</p>

<figure style="text-align: center;">
    <img src="https://github.com/uiceds/project-team492/blob/main/content/images/Random%20Forest%201.png?raw=true">
    <figcaption><strong>Figure XX3:</strong> Comparison between the NO2 values predicted from Random Forest Regressor model and the observed NO2 values for all the training and test data.  </figcaption>
</figure>

<p style="text-align:justify;">  

</p>
__4. Neural Network Algorithm__
<p style="text-align:justify;">     
A neural network is a computational model inspired by the human brain, consisting of layers of interconnected neurons. It is commonly used to learn patterns and relationships in a dataset. This model has been applied to explore its predictive capability compared to other techniques. 
<p style="text-align:justify;">  
The main neural network architecture has been defined using the chain structure, which takes 12 features as input and passes them to the first hidden layer. Three hidden layers have been added with 128, 64, and 32 neurons, along with ReLU activation. A dropout of 0.5 has been kept to drop 50% of the neurons randomly to prevent overfitting. A single neuron outputs the predicted NO2 value. MSE and L1 regularization have been incorporated into the code to minimize the loss function. L1 regularization penalty adds a penalty proportional to the absolute values of the modelâ€™s weights. For this predictive model, RMSE value was 3.12 and R2 value was 0.61.
</p>

<figure style="text-align: center;">
    <img src="https://github.com/uiceds/project-team492/blob/main/content/images/Neural%20Network.png?raw=true">
    <figcaption><strong>Figure XX4:</strong> Comparison between the NO2 values predicted from Neural Network model and the observed NO2 values for all the training and test data. </figcaption>
</figure>
</p>

__Comparison Among Different Predictive Models__
<p style="text-align:justify;">  
The analysis utilized four predictive modeling techniques to predict NO2 concentrations using a dataset of various land use patterns in different locations across the US. The evaluation metrics of all the predictive models used in the analysis are summarized in the table below. The accuracy of the predictive model is as follows: Neural Network model > Random Forest Regression > Decision tree > Linear Regression. Although the linear regression modeling had the least accuracy, the value was still comparably low, which is also considered accurate. Random Forest model with an R2 value of 0.54 suggested a moderate predictive capability, whereas the R2 value of the Neural Network model indicates that it explained 61% of the variance in NO2 concentration. These results highlight the importance of accounting for the non-linear interactions and feature complexities when modeling NO2 concentrations, with Neural Networks demonstrating their strength in capturing such patterns. In the current analysis, the Neural network model did not enhance the predictive capability that much, and the reason could be due to highly non-linear relationships between the features, which reduced the advantage of applying the neural network model. Moreover, the number of observations ~370 may not be enough data to effectively learn complex patterns, and even with L1 penalties, overfitting is an issue. However, further improvements could be made in optimizing the predictive model, especially regarding feature selections, and exploring other advanced ensemble methods to enhance predictive accuracy and generalizability.
</p>

| Sl. No |   Technique of the Predictive Model  | RMSE | R2 |
|:-------|:-------------------------------------|:-----|:---|
|    1   |            Linear Regression         | 4.30 |-3.4|
|    2   |             Decision Tree            |  --  | -- |
|    3   |        Random Forest Regression      | 3.58 |0.54| 
|    4   |             Neural Network           | 3.12 |0.61|
