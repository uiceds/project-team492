## Predictive Modeling {.page_break_before}
   
__Machine learning methods__ 
<p style="text-align:justify;">
Predictive modeling using machine learning was performed using four methods: linear regression, decision tree, random forest regression, and neural network. The dataset, which has 369 data in eight variables, was used for training. The validity of the model was confirmed visually by plotting it linearly and checking the RMSE and R2 values.
</p>

<p style="text-align:justify;">
RMSE is a root mean square error, in which the best value is 0 and the worst value is near infinite [7-8]. As the range of the criteria is infinite, it is difficult to say at which point the data has a good prediction. However, the data is well-trained when the value is near 0. R2 is the proportion of the variance in the dependent variable that is predictable from the independent variables [7-8]. R2 is best when close to 1 and worst in - infinite. Each machine learning method was compared with these criteria to show their effectiveness.
</p>

<p style="text-align:justify;">
__1. Linear regression__  
</p>

<p style="text-align:justify;">
The first method used for machine learning was the linear regression model. A simple linear relationship was used for training, which gave a sense of data analysis and assisted in planning future machine learning methods. The mean squared error was used to minimize the error, and with seven variables used, each variable was multiplied by a random seed and added with random bias initially. The training was performed and plotted using the normalized data, as shown in Figure 4.1. The procedure for coding is shown in the appendix. The accuracy of the machine learning was validated by calculating R2 and RMSE values. R2 value was -3.4 and RMSE value was 4.3.
</p>

<figure style="text-align: center;">
    <img src="https://github.com/uiceds/project-team492/blob/main/content/images/S_linear.png?raw=true" alt="Sample Image">
    <figcaption><strong>Figure 4.1:</strong> Comparison between the NO2 values predicted from Linear Regression model and the observed NO2 values </figcaption>
</figure>

<p style="text-align:justify;">
__2. Decision tree regression__
</p>

<p style="text-align:justify;">
A decision tree is one of the most popular machine learning algorithms, and it is done in either classification or regression. The method is used to create a model that predicts the value of a target variable by learning simple decision rules. Some critical parameters of this algorithm are:
</p>

<p style="text-align:justify;">    
a) n_trees: It specifies the number of trees in the forest. A large number improves the stability and reduces variance but increases computational cost. Considering all these factors, the value of n_trees was considered 369 in this project.
</p>

<p style="text-align:justify;">  
b) max_depth: It limits the minimum number of samples required to split an internal node. This study considers a depth of 20, which is suitable for capturing the complex land use patterns and correlation with NO2 data without overfitting.
</p>

<p style="text-align:justify;">  
c) min_samples_split: It is the minimum number of samples required to split an internal node. The large values prevent overfitting by avoiding overly specific splits. A value of 2 was considered in the model.
</p>
   
</p><p style="text-align:justify;">  
d) min_samples_leaf: The minimum number of samples required to be at a leaf node ensures that leaf nodes represent significant data, improving model generalization. 
</p>

<p style="text-align:justify;">  
Figure 4.2 shows the machine learning result. A typical 80/20 split for training and testing was used to evaluate the model's generalization on unseen data. The detailed code is attached in the appendix. As in linear regression, the accuracy was checked by R2 and RMSE values, which are -0.0 and 5.55, respectively.
</p>

<figure style="text-align: center;">
    <img src="https://github.com/uiceds/project-team492/blob/main/content/images/Decision_regression.png?raw=true" alt="Sample Image">
    <figcaption><strong>Figure 4.2:</strong> Comparison between the NO2 values predicted from Decision Tree Regression and the observed NO2 valuess </figcaption>
</figure>

__3. Random forest Regressor__
<p style="text-align:justify;">   
Random forest is an ensemble learning method that aggregates predictions from multiple decision trees to reduce overfitting and improve generalization. As in decision tree regression, the variables were set as n_trees:200, max_depth: 20, and min_samples_split: 15.
</p>

<p style="text-align:justify;">  
A typical 80/20 split for training and testing was used to evaluate the model's generalization on unseen data. The accuracy was checked by R2 and RMSE values, which are 0.5 and 3.58, respectively. Figure 4.3 shows the relationship between trained NO2 values and observed NO2 values using Random Forest Regressor model.
</p>

<figure style="text-align: center;">
    <img src="https://github.com/uiceds/project-team492/blob/main/content/images/Random%20Forest%201.png?raw=true">
    <figcaption><strong>Figure 4.3:</strong> Comparison between the NO2 values predicted from Random Forest Regressor model and the observed NO2 values for all the training and test data.  </figcaption>
</figure>
 
<p style="text-align:justify;"> 
__4. Neural Network Algorithm__
</p>
   
<p style="text-align:justify;">     
A neural network is a computational model inspired by the human brain, consisting of layers of interconnected neurons. It is commonly used to learn patterns and relationships in a dataset. This model has been applied to explore its predictive capability compared to other techniques. 
</p>

<p style="text-align:justify;">  
The main neural network architecture has been defined using the chain structure, which takes 12 features as input and passes them to the first hidden layer. Three hidden layers have been added with 128, 64, and 32 neurons, along with ReLU activation. A dropout of 0.5 has been kept to drop 50% of the neurons randomly to prevent overfitting. A single neuron outputs the predicted NO2 value. For this predictive model, R2 value was 0.6 and RMSE value was 3.12. Figure 4.4 shows the relationship between trained NO2 values and observed NO2 values using neural network model.
</p>

<figure style="text-align: center;">
    <img src="https://github.com/uiceds/project-team492/blob/main/content/images/Neural%20Network.png?raw=true">
    <figcaption><strong>Figure 4.44:</strong> Comparison between the NO2 values predicted from Neural Network model and the observed NO2 values for all the training and test data. </figcaption>
</figure>

<p style="text-align:justify;"> 
__Comparison Among Different Predictive Models__
</p>

<p style="text-align:justify;">  
The analysis utilized four predictive modeling techniques to predict NO2 concentrations using a dataset of various land use patterns in different locations across the US. The evaluation metrics of all the predictive models used in the analysis are summarized in the table below. The accuracy of the predictive model is as follows: Neural Network model > Random Forest Regression > Linear Regression > Decision Tree Regression. Although the Decision Tree Regression modeling had the least accuracy, the value was still comparably low, which is also considered accurate. Random Forest model with an R2 value of 0.54 suggested a moderate predictive capability. These results highlight the importance of accounting for the non-linear interactions and feature complexities when modeling NO2 concentrations, with Neural Networks demonstrating their strength in capturing such patterns. Plenty of data was used for machine learning in the dataset. The algorithm could be more tuned to enhance the accuracy; currently, simple regression models are utilized without passing complex filters. Also, the methods could be combined, not only training with one method. Still, the R2 and RMSE values are showing promising results. 
</p>

| Sl. No |   Technique of the Predictive Model  | RMSE | R2 |
|:-------|:-------------------------------------|:-----|:---|
|    1   |            Linear Regression         | 4.30 |-3.4|
|    2   |        Decision Tree Regression      | 5.55 |-0.0|
|    3   |        Random Forest Regression      | 3.58 |0.5 | 
|    4   |             Neural Network           | 3.12 |0.6 |
